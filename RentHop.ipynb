{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm\nfrom sklearn.metrics import log_loss\nfrom nltk import word_tokenize\nfrom nltk import FreqDist\nfrom nltk.corpus import stopwords\nimport string\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom geopy.distance import vincenty",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_json(open(\"../input/train.json\",\"r\"))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"time_since_built\"] = pd.datetime.today() - df[\"created\"]\ndf[\"days_since_built\"] = df[\"time_since_built\"].astype('timedelta64[D]').astype(int)\ndf[\"len_descrip\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\n# set bad words for features such as pre-war, prewar. turns out prewar is a positive word\n# bad_words = ['Pre-War','PreWar','Prewar','prewar','pre-war','pre war','Pre War'] lambda x: len([item for item in x if item not in bad_words])\ndf[\"len_features\"] = df[\"features\"].apply(len)\ndf[\"num_photos\"] = df[\"photos\"].apply(len)",
      "execution_count": 55,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "all_descrip = list(df[\"description\"])\nall_descrip_words = ' '.join(all_descrip)\nall_feat = []\nall_feat.extend(df[\"features\"])\n# all_feat_sum = sum(all_feat,[]) this code works but is tooooo slow, the following code is way better, 10 times faster\nall_feat_sum = [item.lower() for sublist in all_feat for item in sublist] # lowercase reduced freq from 1595 to 1294\nprint(len(all_descrip_words),len(all_feat_sum))",
      "execution_count": 22,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "freq = FreqDist(all_feat_sum)\nfreq_df = pd.DataFrame(freq.most_common(len(freq)))\nfreq_df.columns = [['features','frequencies']]\nfreq_df.head()",
      "execution_count": 97,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "high_freq_feat = list(freq_df[:int(len(freq)/3)][\"features\"])\nmed_freq_feat = list(freq_df[int(len(freq)/3):int(len(freq)*2/3)][\"features\"])\nlow_freq_feat = list(freq_df[int(len(freq)*2/3):][\"features\"])\nprint(high_freq_feat[:10],med_freq_feat[:10],low_freq_feat[:10])",
      "execution_count": 124,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df[\"num_high_feat\"] = df[\"features\"].apply(lambda x: len([item for item in x if item.lower() in high_freq_feat]))\ndf[\"num_med_feat\"] = df[\"features\"].apply(lambda x: len([item for item in x if item.lower() in med_freq_feat]))\ndf[\"num_low_feat\"] = df[\"features\"].apply(lambda x: len([item for item in x if item.lower() in low_freq_feat]))\ndf.head()",
      "execution_count": 126,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.barplot(x='features', y = 'frequencies', data = freq_df)\nplt.xticks(rotation=60, fontsize = 10)",
      "execution_count": 28,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df[\"num_badwords\"] = df[\"features\"].apply(lambda x: len([item for item in x if item in bad_words]))",
      "execution_count": 43,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "stop = stopwords.words('english') + list(string.punctuation)\nstop.extend(['br','br/','p','1','2','3','--',\"''\",\"'s\"])\ntokens = word_tokenize(all_descrip_words.lower())\nclean_descrip = [i for i in tokens if i not in stop]\nfreq = FreqDist(clean_descrip)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "n = 50\nfreq_df = pd.DataFrame(freq.most_common(n))\nfreq_df.columns = [['word','frequencies']]\nfreq_df.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sns.barplot(x='word', y = 'frequencies', data = freq_df)\nplt.xticks(rotation=60, fontsize = 10)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# set keywords according to the frequencies of all words\nkey_words = ['new','appliances','renovated','large','great','spacious','beautiful','beautifully','brand','close','top','excellent','recently','luxury','clean','nice','big','kitchen','amenity','safe']\ndf[\"num_keywords\"] = df[\"description\"].apply(lambda x: sum(x.count(i) for i in key_words))",
      "execution_count": 48,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# calculate the distance from time square and downtown world trade center\ntime_square = [40.7589, -73.9851]\nworld_trade_center = [40.7124, -74.0128]\ndf[\"location\"] = list(zip(df[\"latitude\"],df[\"longitude\"]))\ndf[\"dist_ts\"] = df[\"location\"].apply(lambda x: vincenty(time_square, x).miles)\ndf[\"dist_wtc\"] = df[\"location\"].apply(lambda x: vincenty(world_trade_center, x).miles)",
      "execution_count": 45,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()",
      "execution_count": 85,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "num_feats = [\"bedrooms\",  \"dist_wtc\", \"price\",\n             \"num_photos\", \"len_descrip\", \"num_keywords\", \"num_high_feat\",\"num_med_feat\",\"num_low_feat\",\n             \"days_since_built\"] #remove \"dist_ts\",  \"len_features\", \"bathrooms\", \"num_badwords\",\n#X = df[num_feats]\nX = pd.DataFrame(scaler.fit_transform(df[num_feats]), columns=num_feats)\ny = df[\"interest_level\"]\nX.head()",
      "execution_count": 137,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "corrmat = X.corr()\nsns.heatmap(corrmat, vmax=.8, square=True)",
      "execution_count": 130,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33)",
      "execution_count": 138,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "clf = RandomForestClassifier(n_estimators=1000)\nclf.fit(X_train, y_train)\ny_val_pred = clf.predict_proba(X_val)\nlog_loss(y_val, y_val_pred)",
      "execution_count": 139,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "clf.score(X_val,y_val)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "df = pd.read_json(open(\"../input/test.json\", \"r\"))\ndf[\"num_photos\"] = df[\"photos\"].apply(len)\ndf[\"num_features\"] = df[\"features\"].apply(len)\ndf[\"num_description_words\"] = df[\"description\"].apply(lambda x: len(x.split(\" \")))\ndf[\"created\"] = pd.to_datetime(df[\"created\"])\ndf[\"created_year\"] = df[\"created\"].dt.year\ndf[\"created_month\"] = df[\"created\"].dt.month\ndf[\"created_day\"] = df[\"created\"].dt.day\nX = df[num_feats]\n\ny = clf.predict_proba(X)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "labels2idx = {label: i for i, label in enumerate(clf.classes_)}\nlabels2idx",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "sub = pd.DataFrame()\nsub[\"listing_id\"] = df[\"listing_id\"]\nfor label in [\"high\", \"medium\", \"low\"]:\n    sub[label] = y[:, labels2idx[label]]\nsub.to_csv(\"submission_rf.csv\", index=False)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "'''from sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\ngp_fix = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0), optimizer=None)\ngp_fix.fit(X_train, y_train)\ny_val_pred = gp_fix.predict_proba(X_val)\nlog_loss(y_val,y_val_pred)\ngp_opt = GaussianProcessClassifier(kernel=1.0 * RBF(length_scale=1.0))\ngp_opt.fit(X_train, y_train)''''''",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "''''C = 1.0  # SVM regularization parameter\nsvc = svm.SVC(kernel='linear', C=C, probability=True)\nsvc.fit(X_train, y_train)\ny_val_pred = svc.predict_proba(X_val)\nlog_loss(y_val,y_val_pred) #0.76626587505159971''''",
      "execution_count": 142,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "''''rbf_svc = svm.SVC(kernel='rbf', gamma=0.7, C=C, probability=True)\nrbf_svc.fit(X_train, y_train)\ny_val_pred = rbf_svc.predict_proba(X_val)\nlog_loss(y_val,y_val_pred) #0.78343422893897285 very slow and worse than RF''''",
      "execution_count": 143,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "''''poly_svc = svm.SVC(kernel='poly', degree=3, C=C, probability=True)\npoly_svc.fit(X_train, y_train)\ny_val_pred = poly_svc.predict_proba(X_val)\nlog_loss(y_val,y_val_pred) #0.78747043654211291''''",
      "execution_count": 144,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(n_estimators=1000)\ngbc.fit(X_train, y_train)\ny_val_pred = gbc.predict_proba(X_val)\nlog_loss(y_val, y_val_pred) #0.64473894356754169 for n_estimators = 100 0.63055798833692045 for n_estimators = 1000",
      "execution_count": 147,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import AdaBoostClassifier\nabc = AdaBoostClassifier(n_estimators=1000)\nabc.fit(X_train, y_train)\ny_val_pred = abc.predict_proba(X_val)\nlog_loss(y_val, y_val_pred) #1.0844319221850884 for n-estimators = 100 1.0970218291172218 for n-estimators=1000",
      "execution_count": 149,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(max_features=10)\ndtc.fit(X_train, y_train)\ny_val_pred = dtc.predict_proba(X_val)\nlog_loss(y_val, y_val_pred) #12.550469062345963 12.47179278909033 for max_features=10",
      "execution_count": 152,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 2), random_state=1)\nclf.fit(X_train, y_train)\ny_val_pred = clf.predict_proba(X_val)\nlog_loss(y_val, y_val_pred) #0.7796157421558173",
      "execution_count": 155,
      "outputs": [
        {
          "data": {
            "text/plain": "0.76233173622689721"
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}